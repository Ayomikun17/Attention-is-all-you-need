{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7a38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf87ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    # Specifying the dimension of the vector and the vocabulary size\n",
    "    # d_model is a vector of size 512\n",
    "    def __init__(self, d_model:int, vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # PyTorch inbuilt embedding feature\n",
    "        # Embedding takes in the size of dictionary(vocab) and the size of each embedding vector(512)\n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #  Multiplying the weights of the embedding by the Sqrt of d_model\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a9cee",
   "metadata": {},
   "source": [
    "### Positional Encoding \n",
    "Positional encoding vector is added to the embedding to introduce the actual position of each word in the sentence.\n",
    "\n",
    "For the even position of the embedding vector of each word, we use the formula `PE(pos, 2i)` ---> **Add the actual formula**\n",
    "\n",
    "For the odd we use `PE(pos, 2i+1)`  ---> **Add the actual formula**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b37bb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Specifying the vector size, maximum length of sentence and dropout\n",
    "    def __init__(self, d_model:int, seq_len: int, dropout:float ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Creating a matrix of shape(seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        \n",
    "        # Creating a position vector for the words in the sentence\n",
    "        position = torch.arange(0, seq_len, datatype = torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Representing the formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(50000.0) / d_model))\n",
    "        \n",
    "        # Applying to the even positions\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # odd positions\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Batch dimension\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe) # Saves the tensor pe alongside the model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # We specify that we don't want the model to learn the positional encoding \n",
    "        # throughout the process because they are fixed, requires grad: False\n",
    "        x = x + (self.pe[1, x.shape[1], :]).requires_grad(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e358da",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "Suppose we have a batch containing sentences stored as vectors labelled item 1 to item 4 , we find the mean and variance of each item then we calculate the new values(x) for each of the item using their respective mean and variance (layer normalization)\n",
    "\n",
    "**-->Include the formula**\n",
    " \n",
    "We introduce the gamma (multiplicative -> multiplied by x) and the beta (additive -> added to the x). The model uses this to amplify the values(x) when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f037bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    # Eps avoids to big or small numbers for numerical stability and it prevents division by 0\n",
    "    def __init__(self, eps: float = 10** -6): \n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caac08d",
   "metadata": {},
   "source": [
    "### Feed Forward Layer\n",
    "Fully connected layers used in both the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a67b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model:int , d_ff:int, dropout: float):\n",
    "        self().__init__()\n",
    "        \n",
    "        self.linear_1  = nn.Linear(d_model, d_ff) # W1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2  = nn.Linear(d_ff, d_model) # W2 and b2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_1(self.dropout(torch.relu(self.layer_2(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b47dad",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61166935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # h -> number of heads\n",
    "    def __init__(self, d_model:int, h:int, dropout:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0, 'd_model is not divisible by the number of heads'\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        # Matrices for multiplying the Query, Key and Value\n",
    "        self.w_q = nn.Linear(d_model, d_model) \n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -10**9)\n",
    "        \n",
    "        attention_scores = attention_scores.softmax(dim = -1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        return (attention_scores @ value), attention_score\n",
    "        \n",
    "    def forward(self,x, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        # Divide the Q,K,Y to smaller matrices to give each matrix into a different head\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        x = x.transpose(1, 2).contigous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbdaec",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad09cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout:float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada1bbc",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42f7a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward: FeedForwardBlock, dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connection[0](x, lambda a: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eaad3d",
   "metadata": {},
   "source": [
    "The Encoder block is made up of many encoders so we can have up to `N` encoders in the encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1cc82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers:nn.ModuleList):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5502a84",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387130b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
